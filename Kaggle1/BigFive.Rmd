---
title: "Big 5 DYPLRS"
output: html_notebook
---



Load in data

```{r}
library(tidytext)
library(tidyverse)
list.files(path = "/Users/maraoosterbaan/GitHub/BDS_2019/Kaggle1/youtube-personality/transcripts")

transcript_files = dir("/Users/maraoosterbaan/GitHub/BDS_2019/Kaggle1/youtube-personality/transcripts", full.names = TRUE) 
head(transcript_files)


vlogId = basename(transcript_files)
vlogId = str_replace(vlogId, pattern = ".txt$", replacement = "")
head(vlogId)

transcripts_df = tibble(vlogId=vlogId, Text = map_chr(transcript_files, ~ paste(readLines(.x), collapse = "\\n")), filename = transcript_files) %>% 
  select(-filename)

transcripts_df %>% head()

```

Load in personality data
```{r}
pers = read_delim("/Users/maraoosterbaan/GitHub/BDS_2019/Kaggle1/youtube-personality/YouTube-Personality-Personality_impression_scores_train.csv", " ")
```
Load in gender
```{r}
gender = read.delim("/Users/maraoosterbaan/GitHub/BDS_2019/Kaggle1/youtube-personality/YouTube-Personality-gender.csv", head=FALSE, sep=" ", skip = 1)
names(gender) = c('vlogId', 'gender')
head(gender)
```
Join all data
```{r}
# vlogger_df = left_join(gender, pers)
vlogger_df = left_join(gender, pers)
head(vlogger_df)
```

Create test set
```{r}
testset_vloggers = vlogger_df %>% filter(is.na(Extr))
head(testset_vloggers)
```
```{r}
nrc <- get_sentiments("nrc")

vlogger_tokenized<- transcripts_df %>% 
  unnest_tokens(token, Text, token = 'words') %>% 
  anti_join(stop_words, by = c(token = "word"))  %>% 
  inner_join(get_sentiments('nrc'), by = c(token = 'word')) 

count_sentiment <- vlogger_tokenized %>% 
  count(vlogId, sentiment)
```

Spread data

```{r}
sentiment_wide <- spread(count_sentiment, key = sentiment, value = n, fill = 0)

```

Combine
```{r}
vlogger_features  <- inner_join(vlogger_df, sentiment_wide, by = "vlogId")  #%>% 
  #filter(!is.na(Extr))

head(vlogger_features)
```


Correlate

```{r}

cor_matrix_five <- cor(vlogger_features[,7:16])
cor_five <- caret::findCorrelation(cor_matrix_five) + 6

```

Remove positive and negative as independent variables
```{r}
vlogger_features <- vlogger_features %>% 
  select(-negative,-positive)
```


Do the positive and negative variables of the  Bing library (Bing Liu et al. 2004)contain more information (less redundant)

```{r}
bing <- get_sentiments("bing")


vlogger_tokenized_bing <- transcripts_df %>% 
  unnest_tokens(token, Text, token = 'words') %>% 
  anti_join(stop_words, by = c(token = "word"))  %>% 
  inner_join(bing, by = c(token = 'word')) 

count_sentiment_bing <- vlogger_tokenized_bing %>% 
  count(vlogId, sentiment)

sentiment_wide_bing <- spread(count_sentiment_bing, key = sentiment, value = n, fill = 0)

vlogger_features_bing  <- inner_join(vlogger_features, sentiment_wide_bing, by = "vlogId")

cor_matrix_bing <- cor(vlogger_features_bing[,7:16])
cor_bing <- caret::findCorrelation(cor_matrix_bing) + 6


```

There are no correlations > .9, therefore no variables need to excluded.
Fit linear model


```{r}

lm_1 <- lm(formula = cbind(Extr, Agr, Cons, Emot, Open) ~ .-vlogId -gender , data = vlogger_features_bing)
summary(lm_1)


```


Check predictions on testset
```{r}
pred_mlm = predict(lm_1) 

pred_mlm %>% 
  write.csv("version1.csv", col.names = TRUE)
```


Add audio data
```{r}
audio_df <- read_delim("/Users/maraoosterbaan/GitHub/BDS_2019/Kaggle1/youtube-personality/YouTube-Personality-audiovisual_features.csv", delim = " ")

cor_matrix_audio<- cor(audio_df[,-1])
cor_audio <- caret::findCorrelation(cor_matrix_audio) + 1

which(cor_matrix_audio[,14] > .9)

vlogger_features_audio <- audio_df %>%  
  select(-sd.energy) %>% 
  inner_join(vlogger_features_bing, audio_df,by = "vlogId")

```
 
 

 
 
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

