---
title: "Predicting the BIG FIVE "
output: html_notebook
---


*****
##### 1 Preparing the data
*****

1.1 Loading in the transcript data
```{r}
library(tidytext)
library(tidyverse)
list.files(path = "/Users/maraoosterbaan/GitHub/BDS_2019/Kaggle1/youtube-personality/transcripts")

transcript_files = dir("/Users/maraoosterbaan/GitHub/BDS_2019/Kaggle1/youtube-personality/transcripts", full.names = TRUE) 
head(transcript_files)


vlogId = basename(transcript_files)
vlogId = str_replace(vlogId, pattern = ".txt$", replacement = "")
head(vlogId)

transcripts_df = tibble(vlogId=vlogId, Text = map_chr(transcript_files, ~ paste(readLines(.x), collapse = "\\n")), filename = transcript_files) %>% 
  select(-filename)

transcripts_df %>% head()

```

*****
1.2 Loading in the personality data
```{r}
pers = read_delim("/Users/maraoosterbaan/GitHub/BDS_2019/Kaggle1/youtube-personality/YouTube-Personality-Personality_impression_scores_train.csv", " ")
```

*****
1.3 Loading in the gender data
```{r}
gender = read.delim("/Users/maraoosterbaan/GitHub/BDS_2019/Kaggle1/youtube-personality/YouTube-Personality-gender.csv", head=FALSE, sep=" ", skip = 1)
names(gender) = c('vlogId', 'gender')
head(gender)
```

*****
1.4 Joining the data
```{r}
# vlogger_df = left_join(gender, pers)
vlogger_df = left_join(gender, pers)
head(vlogger_df)
```

*****
Creating the test set
```{r}
testset_vloggersId = vlogger_df %>% filter(is.na(Extr))
head(testset_vloggers)
```
```{r}
nrc <- get_sentiments("nrc")

vlogger_tokenized<- transcripts_df %>% 
  unnest_tokens(token, Text, token = 'words') %>% 
  anti_join(stop_words, by = c(token = "word"))  %>% 
  inner_join(get_sentiments('nrc'), by = c(token = 'word')) 

count_sentiment <- vlogger_tokenized %>% 
  count(vlogId, sentiment)
```

******
Spread data

```{r}
sentiment_wide <- spread(count_sentiment, key = sentiment, value = n, fill = 0)

```

******
Combine
```{r}
vlogger_features  <- inner_join(vlogger_df, sentiment_wide, by = "vlogId")  #%>% 
  #filter(!is.na(Extr))

head(vlogger_features)
```


******
Correlate

```{r}

cor_matrix_five <- cor(vlogger_features[,7:16])
cor_five <- caret::findCorrelation(cor_matrix_five) + 6

```

Remove positive and negative as independent variables
```{r}
vlogger_features <- vlogger_features %>% 
  select(-negative,-positive)
```


Do the positive and negative variables of the  Bing library (Bing Liu et al. 2004)contain more information (less redundant)

```{r}
bing <- get_sentiments("bing")


vlogger_tokenized_bing <- transcripts_df %>% 
  unnest_tokens(token, Text, token = 'words') %>% 
  anti_join(stop_words, by = c(token = "word"))  %>% 
  inner_join(bing, by = c(token = 'word')) 

count_sentiment_bing <- vlogger_tokenized_bing %>% 
  count(vlogId, sentiment)

sentiment_wide_bing <- spread(count_sentiment_bing, key = sentiment, value = n, fill = 0)

vlogger_features_bing  <- inner_join(vlogger_features, sentiment_wide_bing, by = "vlogId") 


cor_matrix_bing <- cor(vlogger_features_bing[,7:16])
cor_bing <- caret::findCorrelation(cor_matrix_bing) + 6


```

There are no correlations > .9, therefore no variables need to excluded.

### word count with stop words included

word_count <- transcripts_df %>% 
  unnest_tokens(token, Text, token = 'words') %>% 
  group_by(vlogId) %>%
  tally(name = 'word count')
## word length higher than 6

word_length <- transcripts_df %>% 
  unnest_tokens(token, Text, token = 'words') %>% 
  filter(str_length(token) > 6) %>%
  group_by(vlogId) %>%
  tally(name = 'lengthy words')

##swearwords

swearwods <- read.csv("swear.csv", header = F)

swears <- transcripts_df %>% 
  unnest_tokens(token, Text, token = 'words') %>% 
  inner_join(swearwods, by = c(token = 'V1')) %>%
  group_by(vlogId) %>%
  tally(name = 'swear')

### SELF-REFERENCE ###


selfreference <- transcripts_df %>% 
  unnest_tokens(token, Text, token = 'words') %>% 
  filter(token == "i"  | token ==  "me" | token == "myself" | token == "i'm" | token == "mine" |
           token == "my") %>%
  group_by(vlogId) %>%
  tally(name = 'selfwords') 

###joining the data
finaldata <- left_join(word_length, vlogger_features_bing, by = 'vlogId') %>% 
  left_join(word_count, by = 'vlogId') %>%
  left_join(selfreference, by = 'vlogId') %>%
  left_join(swears, by = "vlogId")

finaldata$swear[is.na(finaldata$swear)] = 0
finaldata$selfwords[is.na(finaldata$selfwords)] = 0 ### look at dplyr how to transform NAs to 0


finaldata <- finaldata %>% 
  select(-gender) %>% select(-vlogId) 


Fit linear model


```{r}

lm_1 <- lm(formula = cbind(Extr, Agr, Cons, Emot, Open) ~ ., data = finaldata)
summary(lm_1)


```


Check predictions on testset
```{r}
testset <- finaldata %>% 
  filter(is.na(Extr))
pred_mlm = predict(lm_1, new = testset) 

final <- cbind(testset_vloggersId[,1],pred_mlm) 
csv_final <- as_data_frame(final)  %>% 
  gather(key = "axis", value = "value", -V1) %>% 
  arrange(V1, axis) %>% 
  unite(Id, V1, axis) %>% 
  write.csv("predictions.csv")
```


```
 
 

 
 
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

